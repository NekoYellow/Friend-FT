# åˆ†æ¡¶-è¯„åˆ†-é‡‡æ ·
# 1. åˆ†æ¡¶ (Bucketing)ï¼šé¦–å…ˆï¼Œè„šæœ¬ä¼šéå†æ‰€æœ‰å¤šè½®å¯¹è¯ï¼Œæ ¹æ®å®ƒä»¬çš„å¯¹è¯è½®æ•°ï¼ˆmessagesåˆ—è¡¨çš„é•¿åº¦ï¼‰
# å°†å®ƒä»¬åˆ†ä¸ºâ€œçŸ­å¯¹è¯â€ã€â€œä¸­ç­‰å¯¹è¯â€ã€â€œé•¿å¯¹è¯â€ä¸‰ä¸ªæ¡¶ã€‚
# 2. è¯„åˆ† (Scoring)ï¼šåœ¨æ¯ä¸ªæ¡¶å†…éƒ¨ï¼Œè„šæœ¬ä¼šä¸ºæ¯ä¸€æ®µå¯¹è¯æ‰“ä¸€ä¸ªâ€œè´¨é‡åˆ†â€ã€‚åˆ†æ•°åŸºäºæˆ‘ä»¬å®šä¹‰çš„å¯å‘å¼
# è§„åˆ™ï¼ˆå¦‚å…³é”®è¯å¯†åº¦ã€<|split|>ä½¿ç”¨é¢‘ç‡ã€è§’è‰²è½¬æ¢é¢‘ç‡ç­‰ï¼‰ã€‚
# 3. é‡‡æ · (Sampling)ï¼šæœ€åï¼Œè„šæœ¬ä¼šæ ¹æ®ä½ è®¾å®šçš„ç›®æ ‡æ¯”ä¾‹ï¼ˆæ¯”å¦‚ï¼Œæˆ‘æƒ³è¦20%çš„çŸ­å¯¹è¯ï¼Œ50%çš„ä¸­ç­‰å¯¹è¯ï¼Œ
# 30%çš„é•¿å¯¹è¯ï¼‰ï¼Œä»æ¯ä¸ªæ¡¶ä¸­æŒ‘é€‰å‡ºåˆ†æ•°æœ€é«˜çš„æ ·æœ¬ï¼Œç»„åˆæˆæœ€ç»ˆçš„ç²¾åæ•°æ®é›†ã€‚

import json
import random
from tqdm import tqdm
import math

# =====================================================================================
# --------------------------------- ã€ expert configuration ã€‘ ---------------------------------
# =====================================================================================

# 1. File Paths
INPUT_FILE = "ft_dataset_multi.jsonl"
OUTPUT_FILE = "ft_dataset.jsonl"

MESSAGE_SPLIT_TOKEN = "<|split|>"

SYSTEM_PROMPT = "ä½ å°†æ‰®æ¼”ç”¨æˆ·çš„å¥½å‹'There'ä¸ç”¨æˆ·åœ¨çº¿èŠå¤©ã€‚"


# 2. ç›®æ ‡æ•°é‡ä¸åˆ†å¸ƒ
TARGET_SAMPLE_COUNT = 6000

LENGTH_BUCKETS = {
    "short": (2, 3),
    "medium": (4, 6),
    "long": (7, 100)
}

TARGET_DISTRIBUTION = {
    "short": 0.50,
    "medium": 0.30,
    "long": 0.20,
}
assert math.isclose(sum(TARGET_DISTRIBUTION.values()), 1.0), "TARGET_DISTRIBUTION çš„æ€»å’Œå¿…é¡»ä¸º 1.0"


# 3. å¸¦æƒé‡çš„å…³é”®è¯å­—å…¸
# ä¸ºä½ å¥½å‹çš„ç‹¬ç‰¹ç”¨è¯åˆ†é…æƒé‡åˆ†æ•°ã€‚
# å»ºè®®ï¼šè¶Šæ˜¯ç‹¬ç‰¹ã€ç½•è§çš„å£å¤´ç¦…ï¼Œæƒé‡è¶Šé«˜ã€‚è¶Šæ˜¯å¸¸ç”¨çš„è¡¨æƒ…æˆ–è¯è¯­ï¼Œæƒé‡è¶Šä½ã€‚
KEYWORD_WEIGHTS = {
    # -- é«˜æƒé‡ (ç‹¬ç‰¹å£å¤´ç¦…/è¡Œä¸º) --
    "ä¸ªä¹ˆ": 8, "å¥³äºº": 8, "ä½•æ„": 8, 
    "è‰æ‹Ÿçš„": 8, "æ— æ‰€è°“": 8,
    
    # -- ä¸­ç­‰æƒé‡ (å¸¸ç”¨è¯) --
    "è¿˜çœŸæ˜¯": 6, "ww": 6, "å‘œå‘œ": 6,
    "å¥½çœ‹": 4, "æ„Ÿè§‰": 4, "å“": 4, "å¦ˆçš„": 6,

    # -- ä½æƒé‡ (å¸¸ç”¨è¡¨æƒ…/è¯­æ°”è¯) --
    "[Awkward]": 2, "[Angry]": 2, "[Sob]": 2, "ğŸ¤”": 2,
    "å•Š": 1
}

# 4. è¯„åˆ†æƒé‡
SCORING_WEIGHTS = {
    "keywords_master_weight": 1.5, # ä½œä¸ºæ‰€æœ‰å…³é”®è¯å¾—åˆ†çš„â€œæ€»ä¹˜æ•°â€
    "split_token_freq": 20,
    "turn_alternation": 10,
    "variance_in_length": 5
}


# =====================================================================================
# --------------------------------- ã€ script logic ã€‘ ---------------------------------
# =====================================================================================

def load_jsonl(file_path):
    """ä»JSONLæ–‡ä»¶åŠ è½½æ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return [json.loads(line) for line in f]

def save_jsonl(data, file_path):
    """å°†æ•°æ®ä¿å­˜ä¸ºJSONLæ–‡ä»¶"""
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

def score_conversation(conversation):
    """ä¸ºä¸€æ•´æ®µå¤šè½®å¯¹è¯è®¡ç®—è´¨é‡åˆ† (ä½¿ç”¨åŠ æƒå…³é”®è¯)"""
    messages = conversation["messages"]
    num_turns = len(messages)
    if num_turns == 0:
        return 0

    assistant_replies = [msg['content'] for msg in messages if msg['role'] == 'assistant']
    if not assistant_replies:
        return 0

    # 1. è®¡ç®—åŠ æƒå…³é”®è¯å¾—åˆ†
    total_keyword_score = 0
    for reply in assistant_replies:
        # ç´¯åŠ åœ¨å›å¤ä¸­æ‰¾åˆ°çš„æ‰€æœ‰å…³é”®è¯çš„æƒé‡
        total_keyword_score += sum(weight for keyword, weight in KEYWORD_WEIGHTS.items() if keyword in reply)
    # æŒ‰æ€»è½®æ•°è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°â€œæ¯è½®å¹³å‡å…³é”®è¯å¾—åˆ†â€
    keyword_score_per_turn = total_keyword_score / num_turns
    
    # 2. <|split|> ä½¿ç”¨é¢‘ç‡
    split_count = sum(reply.count(MESSAGE_SPLIT_TOKEN) for reply in assistant_replies)
    split_freq = split_count / len(assistant_replies)

    # 3. è§’è‰²è½¬æ¢é¢‘ç‡
    alternations = sum(1 for i in range(1, num_turns) if messages[i]['role'] != messages[i-1]['role'])
    turn_alternation_ratio = alternations / (num_turns - 1) if num_turns > 1 else 0

    # 4. å›å¤é•¿åº¦å¤šæ ·æ€§
    reply_lengths = [len(reply) for reply in assistant_replies]
    mean_len = sum(reply_lengths) / len(reply_lengths)
    variance = sum((l - mean_len) ** 2 for l in reply_lengths) / len(reply_lengths)
    variance_score = math.log1p(variance)

    # è®¡ç®—æ€»åˆ†
    total_score = (
        (keyword_score_per_turn * SCORING_WEIGHTS["keywords_master_weight"]) +
        (split_freq * SCORING_WEIGHTS["split_token_freq"]) +
        (turn_alternation_ratio * SCORING_WEIGHTS["turn_alternation"]) +
        (variance_score * SCORING_WEIGHTS["variance_in_length"])
    )
    
    return total_score

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("="*60)
    print("å¯åŠ¨é«˜çº§å¯å‘å¼ç­›é€‰è„šæœ¬ (å¸¦ç‹¬ç«‹å…³é”®è¯æƒé‡)")
    print("="*60)

    # 1. åŠ è½½æ‰€æœ‰å·²ç”Ÿæˆçš„å¤šè½®å¯¹è¯æ•°æ®
    print(f"\n[Step 1/4] ä» {INPUT_FILE} åŠ è½½æ•°æ®...")
    all_conversations = load_jsonl(INPUT_FILE)
    print(f"åŠ è½½äº† {len(all_conversations)} æ®µå®Œæ•´å¯¹è¯ã€‚")

    # 2. è¯„åˆ†å¹¶æ”¾å…¥å¯¹åº”çš„é•¿åº¦æ¡¶
    print("\n[Step 2/4] æ­£åœ¨è¯„åˆ†å¹¶è¿›è¡Œåˆ†æ¡¶...")
    buckets = {name: [] for name in LENGTH_BUCKETS}
    other_bucket = []

    for conv in tqdm(all_conversations, desc="è¯„åˆ†å’Œåˆ†æ¡¶"):
        score = score_conversation(conv)
        conv["messages"] = [{"role": "system", "content": SYSTEM_PROMPT}] + conv["messages"]
        num_turns = len(conv["messages"])
        
        placed = False
        for name, (min_len, max_len) in LENGTH_BUCKETS.items():
            if min_len <= num_turns <= max_len:
                buckets[name].append((score, conv))
                placed = True
                break
        if not placed:
            other_bucket.append((score, conv))
            
    print("åˆ†æ¡¶å®Œæˆã€‚å„æ¡¶å†…æ ·æœ¬æ•°é‡:")
    for name, bucket_items in buckets.items():
        print(f"  - {name.capitalize()} å¯¹è¯: {len(bucket_items)} æ¡")
    print(f"  - å…¶ä»– (ä¸ç¬¦åˆé•¿åº¦å®šä¹‰): {len(other_bucket)} æ¡")

    # 3. ä»æ¯ä¸ªæ¡¶ä¸­æŒ‰æ¯”ä¾‹ç­›é€‰å‡ºåˆ†æ•°æœ€é«˜çš„æ ·æœ¬
    print("\n[Step 3/4] æ­£åœ¨æ ¹æ®ç›®æ ‡åˆ†å¸ƒè¿›è¡Œåˆ†å±‚é‡‡æ ·...")
    final_dataset = []
    for name, percentage in TARGET_DISTRIBUTION.items():
        num_to_take = int(TARGET_SAMPLE_COUNT * percentage)
        buckets[name].sort(key=lambda x: x[0], reverse=True)
        selected_samples = [conv for score, conv in buckets[name][:num_to_take]]
        final_dataset.extend(selected_samples)
        print(f"  - ä» '{name}' æ¡¶ä¸­é‡‡æ ·äº† {len(selected_samples)}/{num_to_take} æ¡ (ç›®æ ‡/å®é™…)ã€‚")

    # 4. ä¿å­˜æœ€ç»ˆçš„æ•°æ®é›†
    print("\n[Step 4/4] æ­£åœ¨æ‰“ä¹±æ•°æ®å¹¶ä¿å­˜...")
    random.shuffle(final_dataset)
    save_jsonl(final_dataset, OUTPUT_FILE)

    print("\n" + "="*60)
    print("ğŸ‰ æœ€ç»ˆç²¾åæ•°æ®é›†æ„å»ºæˆåŠŸï¼")
    print(f"æ€»è®¡ç­›é€‰å‡º {len(final_dataset)} æ¡ç¬¦åˆåˆ†å¸ƒçš„é«˜è´¨é‡å¯¹è¯ã€‚")
    print(f"æ–‡ä»¶å·²ä¿å­˜è‡³: {OUTPUT_FILE}")
    print("="*60)

if __name__ == "__main__":
    main()