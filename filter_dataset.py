# -*- coding: utf-8 -*-

import json
import random
from tqdm import tqdm

# =====================================================================================
# --------------------------------- ã€é…ç½®åŒºã€‘ ---------------------------------
#         è¯·æ ¹æ®ä½ çš„å®é™…æƒ…å†µå’Œåå¥½ï¼Œä»”ç»†ä¿®æ”¹è¿™é‡Œçš„å‚æ•°
# =====================================================================================

# 1. æ–‡ä»¶è·¯å¾„
INPUT_FILE_PATH = "ft_dataset.jsonl"      # ä½ çš„åŸå§‹æ•°æ®é›†æ–‡ä»¶
OUTPUT_FILE_PATH = "ft_dataset_filtered.jsonl" # ç­›é€‰åè¾“å‡ºçš„ç²¾åæ•°æ®é›†æ–‡ä»¶

# 2. ç›®æ ‡æ•°é‡
# ä½ å¸Œæœ›ä»åŸå§‹æ•°æ®ä¸­ç­›é€‰å‡ºå¤šå°‘æ¡é«˜è´¨é‡çš„æ ·æœ¬ï¼Ÿ
TARGET_SAMPLE_COUNT = 3000

# 3. å¥½å‹çš„ç‹¬ç‰¹ç”¨è¯ (ï¼ï¼ï¼å…³é”®ï¼ï¼ï¼)
# åˆ—å‡ºä½ å¥½å‹å¸¸ç”¨ã€å…·æœ‰æ ‡å¿—æ€§çš„è¯è¯­ã€å£å¤´ç¦…æˆ–è¡¨æƒ…ç¬¦å·ã€‚
# åˆ—è¡¨è¶Šä¸°å¯Œï¼Œç­›é€‰çš„é’ˆå¯¹æ€§è¶Šå¼ºã€‚
KEYWORD_LIST = [
    "xs", "ä½•æ„", "è¿˜çœŸæ˜¯", "ww", "å‘œå‘œ", "å¥³äºº", "æ„Ÿè§‰", "å“", "ä¸ªä¹ˆ",
    "ğŸ˜‚", "ğŸ˜…", "ğŸ¤”"
]

# 4. è¯„åˆ†æƒé‡ (ä½ å¯ä»¥è°ƒæ•´ä¸åŒç­–ç•¥çš„é‡è¦æ€§)
# è¿™æ˜¯å¯å‘å¼ç­›é€‰çš„æ ¸å¿ƒï¼Œå†³å®šäº†æˆ‘ä»¬å¦‚ä½•è¯„ä»·ä¸€æ¡æ•°æ®çš„å¥½åã€‚
WEIGHTS = {
    "split_token": 8,   # åŒ…å« <|split|> çš„æ ·æœ¬
    "keywords": 7,      # æ¯å‡ºç°ä¸€ä¸ªå¥½å‹çš„å¸¸ç”¨å…³é”®è¯ï¼Œå°±åŠ åˆ†
    "length": 9,        # å›å¤é•¿åº¦åœ¨ç†æƒ³èŒƒå›´å†…ï¼Œç»™äºˆå¥–åŠ±
    "context": 2        # å¯¹è¯å†å²è¶Šé•¿ï¼Œä¸Šä¸‹æ–‡è¶Šä¸°å¯Œï¼Œç»™äºˆå¥–åŠ±
}

# 5. é•¿åº¦è¯„åˆ†çš„ç†æƒ³èŒƒå›´ (å­—ç¬¦æ•°)
# æˆ‘ä»¬ä¸å¸Œæœ›å›å¤å¤ªçŸ­ï¼ˆå¦‚â€œå—¯â€ï¼‰æˆ–å¤ªé•¿ï¼ˆå¯èƒ½æ˜¯å¤§æ®µå¤åˆ¶ç²˜è´´ï¼‰ã€‚
IDEAL_LENGTH_RANGE = (10, 80)


# =====================================================================================
# --------------------------------- ã€è„šæœ¬ä¸»é€»è¾‘ã€‘ ---------------------------------
#                      ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œä½ ä¸éœ€è¦ä¿®æ”¹ä¸‹é¢çš„ä»£ç 
# =====================================================================================

def load_jsonl(file_path):
    """ä»JSONLæ–‡ä»¶åŠ è½½æ•°æ®"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                # å‡è®¾æ¯è¡Œæ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¸”åŒ…å« "messages" é”®
                data.append(json.loads(line))
            except json.JSONDecodeError:
                print(f"è­¦å‘Šï¼šè·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ: {line.strip()}")
    return data

def save_jsonl(data, file_path):
    """å°†æ•°æ®ä¿å­˜ä¸ºJSONLæ–‡ä»¶"""
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

def score_sample(sample):
    """
    ä¸ºå•ä¸ªæ•°æ®æ ·æœ¬ï¼ˆä¸€æ¡å¤šè½®å¯¹è¯ï¼‰è®¡ç®—åˆ†æ•°ã€‚
    """
    total_score = 0
    
    # ç¡®ä¿ 'messages' é”®å­˜åœ¨ä¸”ä¸ä¸ºç©º
    if not sample.get("messages"):
        return 0

    messages = sample["messages"]
    
    # 1. ä¸Šä¸‹æ–‡è¯„åˆ†ï¼šå¯¹è¯è½®æ¬¡è¶Šå¤šï¼Œä¸Šä¸‹æ–‡è¶Šä¸°å¯Œ
    # messages åˆ—è¡¨çš„é•¿åº¦ä»£è¡¨äº†æ€»çš„å¯¹è¯å›åˆæ•°
    num_turns = len(messages)
    total_score += num_turns * WEIGHTS["context"]

    # æˆ‘ä»¬ä¸»è¦åˆ†ææœ€åä¸€æ¡åŠ©æ‰‹çš„å›å¤ï¼Œè¿™æ˜¯æ¨¡å‹è¦å­¦ä¹ æ¨¡ä»¿çš„é‡ç‚¹
    last_assistant_reply = ""
    for msg in reversed(messages):
        if msg.get("role") == "assistant":
            last_assistant_reply = msg.get("content", "")
            break
            
    if not last_assistant_reply:
        return total_score # å¦‚æœæ²¡æœ‰åŠ©æ‰‹å›å¤ï¼Œåˆ™åªè®¡ç®—ä¸Šä¸‹æ–‡åˆ†æ•°

    # 2. <|split|> è¡Œä¸ºè¯„åˆ†
    if "<|split|>" in last_assistant_reply:
        total_score += WEIGHTS["split_token"]

    # 3. å…³é”®è¯è¯„åˆ†
    keyword_count = sum(1 for keyword in KEYWORD_LIST if keyword in last_assistant_reply)
    total_score += keyword_count * WEIGHTS["keywords"]

    # 4. é•¿åº¦è¯„åˆ†
    reply_length = len(last_assistant_reply)
    if IDEAL_LENGTH_RANGE[0] <= reply_length <= IDEAL_LENGTH_RANGE[1]:
        total_score += WEIGHTS["length"]
        
    return total_score

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("="*50)
    print("å¼€å§‹æ‰§è¡Œå¯å‘å¼ç­›é€‰è„šæœ¬...")
    print(f"è¾“å…¥æ–‡ä»¶: {INPUT_FILE_PATH}")
    print(f"ç›®æ ‡æ ·æœ¬æ•°: {TARGET_SAMPLE_COUNT}")
    print("="*50)

    # åŠ è½½æ•°æ®
    print("\n[æ­¥éª¤ 1/4] æ­£åœ¨åŠ è½½åŸå§‹æ•°æ®é›†...")
    all_data = load_jsonl(INPUT_FILE_PATH)
    if not all_data:
        print("é”™è¯¯ï¼šæœªåŠ è½½åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼ã€‚")
        return
    print(f"åŠ è½½å®Œæˆï¼Œå…± {len(all_data)} æ¡æ•°æ®ã€‚")

    # ä¸ºæ¯æ¡æ•°æ®è¯„åˆ†
    print("\n[æ­¥éª¤ 2/4] æ­£åœ¨ä¸ºæ¯æ¡æ•°æ®è¯„åˆ†...")
    scored_data = []
    for sample in tqdm(all_data, desc="è¯„åˆ†è¿›åº¦"):
        score = score_sample(sample)
        scored_data.append((score, sample))
    print("è¯„åˆ†å®Œæˆã€‚")

    # æŒ‰åˆ†æ•°æ’åº
    print("\n[æ­¥éª¤ 3/4] æ­£åœ¨æ ¹æ®åˆ†æ•°æ’åº...")
    scored_data.sort(key=lambda x: x[0], reverse=True)
    print("æ’åºå®Œæˆã€‚")

    # ç­›é€‰å‡ºåˆ†æ•°æœ€é«˜çš„ N æ¡
    print(f"\n[æ­¥éª¤ 4/4] æ­£åœ¨ç­›é€‰åˆ†æ•°æœ€é«˜çš„ {TARGET_SAMPLE_COUNT} æ¡æ•°æ®...")
    
    # é˜²æ­¢ç›®æ ‡æ•°é‡è¶…è¿‡æ€»æ•°
    actual_count = min(TARGET_SAMPLE_COUNT, len(scored_data))
    
    # æå–æ ·æœ¬ï¼Œå»é™¤åˆ†æ•°
    filtered_data = [sample for score, sample in scored_data[:actual_count]]
    
    # ä¸ºäº†ä¿è¯å¤šæ ·æ€§ï¼Œæœ€åå†éšæœºæ‰“ä¹±ä¸€æ¬¡
    random.shuffle(filtered_data)
    
    print("ç­›é€‰å®Œæˆã€‚")

    # ä¿å­˜åˆ°æ–°æ–‡ä»¶
    save_jsonl(filtered_data, OUTPUT_FILE_PATH)
    
    print("\n" + "="*50)
    print("ğŸ‰ è„šæœ¬æ‰§è¡ŒæˆåŠŸï¼")
    print(f"å·²ä» {len(all_data)} æ¡åŸå§‹æ•°æ®ä¸­ç­›é€‰å‡º {len(filtered_data)} æ¡é«˜è´¨é‡æ•°æ®ã€‚")
    print(f"ç²¾åæ•°æ®é›†å·²ä¿å­˜åˆ°: {OUTPUT_FILE_PATH}")
    print("ç°åœ¨ä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ–°æ–‡ä»¶è¿›è¡Œå¾®è°ƒäº†ã€‚")
    print("="*50)

if __name__ == "__main__":
    main()